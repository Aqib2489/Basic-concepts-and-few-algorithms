{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCfoYjrmGtro"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W4niTXoGtrq"
      },
      "source": [
        "The Breast Cancer Wisconsin (Diagnostic) dataset is a well-known and straightforward binary classification dataset in the field of machine learning. It is frequently utilized to showcase the performance of various algorithms. This dataset is included within the sklearn package and comprises a total of 569 samples, with measurements of 30 distinct features pertaining to breast cancer cell nuclei. The primary objective is to predict whether a tumor is malignant or benign. A comprehensive guide to this dataset can be accessed at the following link: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset\n",
        "\n",
        "For this assignment, you are required to apply clustering algorithms and Principal Component Analysis (PCA) to the dataset and address the subsequent questions. The following code snippet loads the dataset, divide it into training and testing sets and normalize the features for subsequent analysis. Please do not change the following code. **Please note that for this assignment, you are not required to create a validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD0QTG4dGtrs"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state=20)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying a few rows of the training dataset\n",
        "print(\"Training Data:\")\n",
        "print(pd.DataFrame(X_train, columns=data.feature_names).head())\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "\n",
        "# Displaying a few rows of the testing dataset\n",
        "print(\"\\nTesting Data:\")\n",
        "print(pd.DataFrame(X_test, columns=data.feature_names).head())\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap1gjU6pK1fv",
        "outputId": "886f4f6d-70c1-4734-a5f7-8a3a39d19061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0    -1.309889      0.377965       -1.292508  -1.069139         0.615460   \n",
            "1    -0.786488      0.435374       -0.794114  -0.720053        -0.639346   \n",
            "2    -0.406847     -1.651995       -0.455673  -0.449519        -0.619938   \n",
            "3    -0.730658     -1.130727       -0.706683  -0.702475         0.275072   \n",
            "4     1.379695      1.590430        1.487140   1.300031         1.891913   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0         -0.483415       -0.808067            -0.850139      -0.298225   \n",
            "1         -0.710875       -0.457596            -0.750320      -1.147102   \n",
            "2         -0.859161       -0.785815            -0.623208      -0.792788   \n",
            "3          0.167257       -0.267742            -0.574688       0.074543   \n",
            "4          1.441261        1.227533             2.482059      -0.604559   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                0.988942  ...     -1.201222       0.938228        -1.184336   \n",
            "1                0.420360  ...     -0.665438       0.549570        -0.684537   \n",
            "2                0.030036  ...     -0.533553      -1.422750        -0.562828   \n",
            "3                0.809147  ...     -0.819991      -0.977648        -0.838072   \n",
            "4                1.488372  ...      0.843000       1.125300         1.004349   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0   -0.962459          1.015581          -0.564652        -0.843676   \n",
            "1   -0.631839          1.696216          -0.208973         0.172327   \n",
            "2   -0.545744         -0.039177          -0.602761        -0.844935   \n",
            "3   -0.738223          0.100556          -0.261121        -0.444605   \n",
            "4    0.744095          0.312409           0.206209        -0.148496   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0             -0.755907        0.012700                 0.516106  \n",
            "1             -0.480857       -0.069888                 0.292830  \n",
            "2             -0.489292       -0.160904                 0.665153  \n",
            "3             -0.693275       -0.912626                -0.118376  \n",
            "4              1.101651       -0.942964                 0.511393  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Shape of X_train: (284, 30)\n",
            "Shape of y_train: (284,)\n",
            "\n",
            "Testing Data:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0    -0.962351      0.235593       -0.914180  -0.861775         0.899116   \n",
            "1     0.405471      0.003663        0.314684   0.274471        -1.325347   \n",
            "2     1.248496      1.319463        1.313891   1.179183         0.764752   \n",
            "3    -0.602250      2.035920       -0.621267  -0.593712        -0.879708   \n",
            "4     0.009082      1.020939        0.027412  -0.131743        -0.113836   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0          0.488131       -0.065224            -0.505699      -0.479073   \n",
            "1         -0.892957       -0.566963            -0.510753      -0.925656   \n",
            "2          1.623533        1.679678             1.913467       1.421675   \n",
            "3         -0.734609       -0.585190            -0.738695      -0.674684   \n",
            "4          0.520409        0.254592             0.402784       1.613595   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                0.932084  ...     -0.937452      -0.085830        -0.904967   \n",
            "1               -1.248505  ...      0.344309       0.955967         0.255829   \n",
            "2               -0.089827  ...      1.424120       1.333337         1.570160   \n",
            "3               -0.232741  ...     -0.648953       1.957448        -0.652710   \n",
            "4                0.272836  ...      0.127934       1.468803         0.229306   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0   -0.821848          0.055481           0.027701        -0.100932   \n",
            "1    0.198238         -1.075905          -0.469047        -0.228702   \n",
            "2    1.400394          0.758653           1.168950         1.577329   \n",
            "3   -0.621959         -0.516974          -0.436287        -0.438077   \n",
            "4   -0.114563          1.065164           1.036573         0.463307   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0             -0.430565       -0.447434                 0.215655  \n",
            "1             -0.435563       -0.315967                -0.887177  \n",
            "2              1.112584        0.966680                 0.852493  \n",
            "3             -0.662349       -0.713740                -0.083029  \n",
            "4              0.993881        3.124089                 1.134092  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Shape of X_test: (285, 30)\n",
            "Shape of y_test: (285,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observing testing dataset\n",
        "print(\"Example of Testing dataset\",X_test[284])\n",
        "print(\"Label of example\",y_test[284])\n",
        "\n",
        "# Observing training datatset\n",
        "print(\"Example of Training datatset\",X_train[283])\n",
        "print(\"Label of example\",y_train[283])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IEmd4HNMUeB",
        "outputId": "c7b3172c-d775-4880-892c-7b5cb8f7e4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of Testing dataset [ 1.67559157  0.0679602   1.56772148  1.69004032 -1.19098342 -0.33323099\n",
            "  0.30193729  0.7222055   0.48052816 -1.78174259  0.62707174 -0.94751083\n",
            "  0.4335168   0.72474738 -0.84520638 -0.62940947 -0.19114284  0.0842971\n",
            "  0.13027108 -0.79881947  1.4055736  -0.43739676  1.26662608  1.37745857\n",
            " -1.21113087 -0.64956075 -0.06735751  0.45190424  0.31102964 -1.41502759]\n",
            "Label of example 0\n",
            "Example of Training datatset [-0.445928   -0.06752358 -0.41377029 -0.4747868  -0.63860001  0.01156648\n",
            "  0.15398403 -0.12057312 -0.9773269  -0.10980428 -0.15175017  0.3814993\n",
            "  0.13920215 -0.27956743  0.89013833  0.87931274  0.80675296  1.16209828\n",
            "  0.62539525  0.15635109 -0.59125278 -0.53415817 -0.5363061  -0.58014688\n",
            " -1.02181529 -0.33867557 -0.16901379 -0.32669872 -1.26320481 -0.67568267]\n",
            "Label of example 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27e6jdohGtrt"
      },
      "source": [
        "# Clustering Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEWZI7lDGtrt"
      },
      "source": [
        "1.1 In this question, you will apply k-means algorithm to the **test set** directly. Report the **accuracy** of the algorithm by comparing the results to the known labels (use accuracy_score function in sklearn). **Note**: You are allowed to use sklearn package for algorithm implementation in this question. As the k-means algorithm is an unsupervised learning method, it does not utilize labels from the data. Consequently, the categories might be flipped (0 becomes 1, and 1 becomes 0). To address this issue, you can calculate both accuracy_score(y_test, kmeans.labels_) and accuracy_score(y_test, 1 - kmeans.labels_), then report the higher value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khSVmei0Gtrt",
        "outputId": "6d78be51-ada0-4703-a176-e3de203a62f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Original Labels): 0.9123\n",
            "Accuracy (Flipped Labels): 0.0877\n",
            "Higher Accuracy: 0.9123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Apply k-means algorithm to the test set\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans.fit(X_test)\n",
        "\n",
        "# Predicted labels (original and flipped)\n",
        "y_pred = kmeans.labels_\n",
        "y_pred_flipped = 1 - kmeans.labels_\n",
        "\n",
        "# Calculate accuracy scores for both label assignments\n",
        "accuracy_original = accuracy_score(y_test, y_pred)\n",
        "accuracy_flipped = accuracy_score(y_test, y_pred_flipped)\n",
        "\n",
        "# Report the higher accuracy score\n",
        "higher_accuracy = max(accuracy_original, accuracy_flipped)\n",
        "\n",
        "print(\"Accuracy (Original Labels): {:.4f}\".format(accuracy_original))\n",
        "print(\"Accuracy (Flipped Labels): {:.4f}\".format(accuracy_flipped))\n",
        "print(\"Higher Accuracy: {:.4f}\".format(higher_accuracy))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_lgOeBDGtru"
      },
      "source": [
        "1.2 In this question, you will apply the gaussian mixture model (GMM) to the **test set** directly. Report the **accuracy** of the algorithm by comparing the results to the known labels (use accuracy_score function in sklearn). **Note**: You are allowed to use sklearn package for algorithm implementation in this question. As the GMM is an unsupervised learning method, it does not utilize labels from the data. Consequently, the categories might be flipped (0 becomes 1, and 1 becomes 0). To address this issue, you can calculate both accuracy_score(y_test, kmeans.labels_) and accuracy_score(y_test, 1 - kmeans.labels_), then report the higher value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R93qlmqiGtru",
        "outputId": "f124ca15-87a0-44cc-bfcc-4eb9e89e1422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Original Labels): 0.9333\n",
            "Accuracy (Flipped Labels): 0.0667\n",
            "Higher Accuracy: 0.9333\n"
          ]
        }
      ],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Apply GMM to the test set\n",
        "gmm = GaussianMixture(n_components=2, random_state=0)\n",
        "gmm.fit(X_test)\n",
        "\n",
        "# Predicted probabilities for each cluster\n",
        "probabilities = gmm.predict_proba(X_test)\n",
        "\n",
        "#Assign labels based on the cluster with the highest probability\n",
        "y_pred = np.argmax(probabilities, axis=1)\n",
        "y_pred_flipped = 1 - y_pred  # Flip labels\n",
        "\n",
        "# Calculate accuracy scores for both label assignments\n",
        "accuracy_original = accuracy_score(y_test, y_pred)\n",
        "accuracy_flipped = accuracy_score(y_test, y_pred_flipped)\n",
        "\n",
        "# Report the higher accuracy score\n",
        "higher_accuracy = max(accuracy_original, accuracy_flipped)\n",
        "\n",
        "print(\"Accuracy (Original Labels): {:.4f}\".format(accuracy_original))\n",
        "print(\"Accuracy (Flipped Labels): {:.4f}\".format(accuracy_flipped))\n",
        "print(\"Higher Accuracy: {:.4f}\".format(higher_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxKkaIN-Gtru"
      },
      "source": [
        "1.3 Compare and comment on the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4x7q6DHGtrv"
      },
      "outputs": [],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "\n",
        "K-means algorithm seems to be a little bit lesser efficient in clustering when compared to GMM. Reported accuracy for GMM using the same dataset was 0.933 while\n",
        "K-means algorithm showed a accuracy of 0.912. However, it should be noted that both models have done pretty well job on the given dataset.\n",
        "\n",
        "It should be noted that GMM is considered to be more robust and efficient as it offers greater flexibility in modeling complex cluster structures,\n",
        "provides probabilistic assignments, and is less sensitive to initialization compared to K-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MpMUzzoGtrv"
      },
      "source": [
        "# Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOitB7hMGtrv"
      },
      "source": [
        "2.1 Apply Principal Component Analysis (PCA) to the **training set** using **numpy package** only. Keep only one principal component. Report the percentage of variance explained by the first component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StGwu4fMGtrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fea4a8-9270-49ad-c6c3-b0aa235eb4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Variance Explained by the First Principal Component: 45.39%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "import numpy as np\n",
        "\n",
        "# Standardized training data (already obtained from StandardScaler)\n",
        "X_train_standardized = X_train\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "cov_matrix = np.cov(X_train_standardized, rowvar=False)\n",
        "\n",
        "# Perform eigendecomposition\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Keep only the first principal component\n",
        "first_principal_component = eigenvectors[:, 0]\n",
        "\n",
        "# Calculate the percentage of variance explained\n",
        "explained_variance_ratio = eigenvalues[0] / np.sum(eigenvalues)\n",
        "percentage_variance_explained = explained_variance_ratio * 100\n",
        "\n",
        "print(\"Percentage of Variance Explained by the First Principal Component: {:.2f}%\".format(percentage_variance_explained))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpoLiG2OGtrw"
      },
      "source": [
        "2.2 Apply Principal Component Analysis (PCA) to the **training set** using **sklearn** package. Keep only one principal component. Report the percentage of variance explained by the first component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tonmA4_bGtrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b975cf-49fa-4145-83a7-d7e39e77913a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Variance Explained by the First Principal Component: 45.39%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance and fit it to the standardized training data\n",
        "pca = PCA(n_components=1)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# Percentage of variance explained by the first component\n",
        "variance_explained = pca.explained_variance_ratio_[0] * 100\n",
        "\n",
        "print(\"Percentage of Variance Explained by the First Principal Component: {:.2f}%\".format(variance_explained))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dsULNy3Gtrw"
      },
      "source": [
        "2.2 Build a basic Support Vector Machine (SVM) model using both the original normalized features and features transformed by Principal Component Analysis (PCA), retaining 1, 5, 10, and 30 principal components. Compare the performance of these models on the test set. How does the number of principal components affect the performance of the model? Note: You are allowed to use the sklearn package for this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPY8vroSGtrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c869a5-598a-4483-c9d1-61b473444b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Original Features: 0.9719\n",
            "Accuracy with 1 Principal Component: 0.9193\n",
            "Accuracy with 5 Principal Components: 0.9614\n",
            "Accuracy with 10 Principal Components: 0.9789\n",
            "Accuracy with 30 Principal Components: 0.9719\n"
          ]
        }
      ],
      "source": [
        "# TODO: Please provide your answers here. You can add more cells for code or texts if needed.\n",
        "# Don't delete these two comments so that it would be easier for me to locate your answers.\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to build and evaluate SVM models\n",
        "def build_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    model = SVC(kernel='linear', random_state=0)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate and return accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Original normalized features\n",
        "accuracy_original = build_and_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "print(\"Accuracy with Original Features: {:.4f}\".format(accuracy_original))\n",
        "\n",
        "# Apply PCA with 1 principal component\n",
        "pca_1 = PCA(n_components=1)\n",
        "X_train_pca_1 = pca_1.fit_transform(X_train)\n",
        "X_test_pca_1 = pca_1.transform(X_test)\n",
        "accuracy_pca_1 = build_and_evaluate_model(X_train_pca_1, X_test_pca_1, y_train, y_test)\n",
        "print(\"Accuracy with 1 Principal Component: {:.4f}\".format(accuracy_pca_1))\n",
        "\n",
        "# Apply PCA with 5 principal components\n",
        "pca_5 = PCA(n_components=5)\n",
        "X_train_pca_5 = pca_5.fit_transform(X_train)\n",
        "X_test_pca_5 = pca_5.transform(X_test)\n",
        "accuracy_pca_5 = build_and_evaluate_model(X_train_pca_5, X_test_pca_5, y_train, y_test)\n",
        "print(\"Accuracy with 5 Principal Components: {:.4f}\".format(accuracy_pca_5))\n",
        "\n",
        "# Apply PCA with 10 principal components\n",
        "pca_10 = PCA(n_components=10)\n",
        "X_train_pca_10 = pca_10.fit_transform(X_train)\n",
        "X_test_pca_10 = pca_10.transform(X_test)\n",
        "accuracy_pca_10 = build_and_evaluate_model(X_train_pca_10, X_test_pca_10, y_train, y_test)\n",
        "print(\"Accuracy with 10 Principal Components: {:.4f}\".format(accuracy_pca_10))\n",
        "\n",
        "# Apply PCA with 30 principal components\n",
        "pca_30 = PCA(n_components=30)\n",
        "X_train_pca_30 = pca_30.fit_transform(X_train)\n",
        "X_test_pca_30 = pca_30.transform(X_test)\n",
        "accuracy_pca_30 = build_and_evaluate_model(X_train_pca_30, X_test_pca_30, y_train, y_test)\n",
        "print(\"Accuracy with 30 Principal Components: {:.4f}\".format(accuracy_pca_30))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKMOR_YncsxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "05284eed26b7c4d19e101a962ad50a81c80866d9b42ecab1d4657c0e9d113e09"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}